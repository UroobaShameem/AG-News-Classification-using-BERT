{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRIDBwKfULgL"
      },
      "source": [
        "### Installing Necessary Packages\n",
        "\n",
        "This cell installs the required libraries for Natural Language Processing (NLP), machine learning, and data visualization. If you already have the packages installed, you can skip this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi73BAfNTP1f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers nltk datasets numpy seaborn pandas scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fr34J29Ubpo"
      },
      "source": [
        "### Importing Dependencies\n",
        "\n",
        "We import all the necessary libraries and modules. These include:\n",
        "\n",
        "- `pandas`: For loading and manipulating datasets.\n",
        "- `seaborn` and `matplotlib`: For visualizations.\n",
        "- `transformers`: For pre-trained NLP models like BERT.\n",
        "- `nltk`: For text preprocessing tasks such as removing stopwords.\n",
        "- `datasets`: To handle datasets efficiently in a format compatible with the Hugging Face models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSxbwF6kUoXz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhWoRSIHUsVB"
      },
      "source": [
        "### Loading the Training and Test Datasets\n",
        "\n",
        "We load the training and test datasets directly from CSV files (`train.csv` and `test.csv`). The `Class Index` column containing class labels is renamed to `label` for consistency. The goal is to ensure that the datasets are structured correctly for further preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0XnlVvPU_sG"
      },
      "outputs": [],
      "source": [
        "# Load the train dataset\n",
        "train_df = pd.read_csv(\"data/train.csv\", on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Load the test dataset\n",
        "test_df = pd.read_csv(\"data/test.csv\", on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Rename the class label column for consistency\n",
        "train_df = train_df.rename(columns={'Class Index':'label'})\n",
        "test_df = test_df.rename(columns={'Class Index':'label'})\n",
        "\n",
        "# Check the shapes of the dataframes\n",
        "print(train_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM7QZCTJ-seO"
      },
      "outputs": [],
      "source": [
        "# Adjust labels in train and test datasets to be zero-indexed\n",
        "train_df['label'] = train_df['label'] - 1\n",
        "test_df['label'] = test_df['label'] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkirFTGSVDAi"
      },
      "source": [
        "### Data Statistics Visualization\n",
        "\n",
        "To ensure that the dataset is balanced across class labels, we generate a bar plot showing the frequency of each class in the training data. This helps identify any class imbalance issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIG4HFESVM8T"
      },
      "outputs": [],
      "source": [
        "# Visualize the class distribution in the training dataset\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.countplot(x=train_df['label'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQykYExgVP-i"
      },
      "source": [
        "### Checking for Null Values\n",
        "\n",
        "We check for any missing or null values in the datasets. This ensures data quality and helps us avoid potential errors in the following steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ZOWmKNVXqq"
      },
      "outputs": [],
      "source": [
        "# Check for missing or null values in the training dataset\n",
        "train_df.info()\n",
        "\n",
        "# Check for missing or null values in the test dataset\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxKyHqzVbIQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ph2xcUXVgKz"
      },
      "source": [
        "#Data Preprocessing:\n",
        "### Concatenating Title and Description\n",
        "\n",
        "In this step, we concatenate the `Title` and `Description` columns into a single `text` column for both training and test datasets. This combined column is used as input for the model since it contains all relevant textual information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETUPEelpVmhN"
      },
      "outputs": [],
      "source": [
        "# Concatenate Title and Description columns for the training dataset\n",
        "train_df['text'] = train_df['Title'] + train_df['Description']\n",
        "train_df.drop(columns=['Title', 'Description'], axis=1, inplace=True)\n",
        "\n",
        "# Concatenate Title and Description columns for the test dataset\n",
        "test_df['text'] = test_df['Title'] + test_df['Description']\n",
        "test_df.drop(columns=['Title', 'Description'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaq6ZweAVsT2"
      },
      "source": [
        "### Removing Punctuation\n",
        "\n",
        "We define a function to remove punctuation and special characters from the text. This cleaning step improves model performance by eliminating noise in the input data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt0ogGwsVwCd"
      },
      "outputs": [],
      "source": [
        "# Function to remove punctuation and unwanted characters from the text\n",
        "def remove_punctuations(text):\n",
        "    if isinstance(text, (str, bytes)):\n",
        "        text = re.sub(r'[\\\\-]', ' ', text)\n",
        "        text = re.sub(r'[,.?;:\\'(){}!|0-9]', '', text)\n",
        "        return text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Apply punctuation removal for both train and test datasets\n",
        "train_df['text'] = train_df['text'].apply(remove_punctuations)\n",
        "test_df['text'] = test_df['text'].apply(remove_punctuations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQiRxjt1WBWq"
      },
      "source": [
        "### Removing Stopwords\n",
        "\n",
        "Stopwords like \"the\", \"is\", and \"and\" are common but carry little information in text classification tasks. We remove stopwords to help the model focus on more meaningful words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6XBeKxOWJr4"
      },
      "outputs": [],
      "source": [
        "# Download stopwords if not already available\n",
        "stopw = stopwords.words('english')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    clean_text = []\n",
        "    for word in text.split(' '):\n",
        "        if word not in stopw:\n",
        "            clean_text.append(word)\n",
        "    return ' '.join(clean_text)\n",
        "\n",
        "# Apply stopword removal to both train and test datasets\n",
        "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
        "test_df['text'] = test_df['text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGENOFkJW4Ar"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "We use a pre-trained BERT tokenizer to convert the text into token representations that can be processed by the BERT model. The `pipeline` function tokenizes both the training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scKt6BGGW_Ux"
      },
      "outputs": [],
      "source": [
        "# Define the model name and load the tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to tokenize text\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# Convert Pandas dataframe to Hugging Face dataset and tokenize it\n",
        "def pipeline(dataframe):\n",
        "    dataset = Dataset.from_pandas(dataframe, preserve_index=False)\n",
        "    tokenized_ds = dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_ds = tokenized_ds.remove_columns('text')\n",
        "    return tokenized_ds\n",
        "\n",
        "# Tokenize the train and test datasets\n",
        "tokenized_train = pipeline(train_df)\n",
        "tokenized_test = pipeline(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2qFcgZYXDOc"
      },
      "source": [
        "#Tokenization using Pre-built Tokenizer\n",
        "\n",
        "The pre-trained bert-base-uncased tokenizer is used to convert the text into token representations suitable for BERT models.\n",
        "\n",
        "The text is tokenized into subword units, which the model can process, and stored in a dataset format that the model can handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6x-r0XKXMjn"
      },
      "outputs": [],
      "source": [
        "model_name='bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "def pipeline(dataframe):\n",
        "    dataset = Dataset.from_pandas(dataframe, preserve_index=False)\n",
        "    tokenized_ds = dataset.map(preprocess_function, batched=True)\n",
        "    tokenized_ds = tokenized_ds.remove_columns('text')\n",
        "    return tokenized_ds\n",
        "\n",
        "tokenized_train = pipeline(train_df)\n",
        "tokenized_test = pipeline(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyqjqYTaXPlH"
      },
      "source": [
        "### Load Pre-trained BERT Model and Set Training Arguments\n",
        "\n",
        "We load a pre-trained BERT model for sequence classification and set the training arguments. These arguments include hyperparameters such as batch size, learning rate, number of epochs, and gradient accumulation steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51kTHnMA3VCj"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model for sequence classification\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
        "\n",
        "# Set training arguments for the Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Where to save the model\n",
        "    save_strategy='epoch',           # Save model after each epoch\n",
        "    evaluation_strategy='no',        # No evaluation during training\n",
        "    logging_strategy='no',           # Disable logging of loss/metrics\n",
        "    learning_rate=2e-5,              # Learning rate\n",
        "    per_device_train_batch_size=16,  # Batch size\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    weight_decay=0.01,               # Weight decay for regularization\n",
        "    report_to=\"none\",                # Disable wandb or any external logging\n",
        "    #gradient_accumulation_steps=2,   # Accumulate gradients over 2 steps\n",
        "    log_level=\"error\"                # Suppress most logs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah6eXtU8XahC"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "We initialize the `Trainer` and start training the model on the tokenized training dataset. The model's parameters are fine-tuned using the specified training arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTs675qMXjE6"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5M7NSbw9_0X"
      },
      "outputs": [],
      "source": [
        "print(train_df['label'].unique())  # Should return something like [0, 1, 2, 3]\n",
        "print(test_df['label'].unique())   # Should return the same range\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuOaHKNjXmEl"
      },
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "After training, we use the trained model to make predictions on the test dataset. We then calculate and print classification metrics such as precision, recall, and F1-score to evaluate the model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVCmii1qXvQT"
      },
      "outputs": [],
      "source": [
        "# Import necessary evaluation tools\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "preds = trainer.predict(tokenized_test)\n",
        "preds_flat = [np.argmax(x) for x in preds[0]]\n",
        "\n",
        "# Generate a classification report\n",
        "print(classification_report(test_df['label'], preds_flat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X5tEvHpXwWc"
      },
      "source": [
        "#Comparing Predictions on Sample Test Data\n",
        "A manual comparison between the modelâ€™s predictions and actual class labels is made for a few random samples from the test dataset.\n",
        "\n",
        "This allows for quick visual inspection of the model's performance on individual cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ_cod9VX29g"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class_labels=['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "\n",
        "num=random.randint(0,len(test_df)-1)\n",
        "tokenized_test = pipeline(test_df[num:num+10]).remove_columns('label')\n",
        "preds=trainer.predict(tokenized_test)\n",
        "preds_flat = [np.argmax(x) for x in preds[0]]\n",
        "\n",
        "print('Prediction\\tActual\\n----------------------')\n",
        "for i in range(len(preds_flat)):\n",
        "    print(class_labels[preds_flat[i]], ' ---> ', class_labels[test_df['label'].values[num+i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpEWOWqiX85u"
      },
      "source": [
        "### Save the Model\n",
        "\n",
        "We save the trained model to disk. This allows us to reuse the model later without retraining it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs5rNspCYA3d"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCC1wWEwYG3U"
      },
      "source": [
        "#Loading the Saved Model\n",
        "Once saved, the model can be reloaded at any time for further use. The loaded model can be used to make new predictions or for additional fine-tuning if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0EtubsXYJY0"
      },
      "outputs": [],
      "source": [
        "# Reload the saved model from disk\n",
        "model = AutoModelForSequenceClassification.from_pretrained('models')\n",
        "\n",
        "# Re-initialize the Trainer with the loaded model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}